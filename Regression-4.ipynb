{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1. What is Lasso Regression, and how does it differ from other regression techniques?\n",
    "Lasso Regression (Least Absolute Shrinkage and Selection Operator) is a type of linear regression that uses regularization to reduce the complexity of the model and prevent overfitting. It achieves this by adding a penalty term to the loss function, which is proportional to the absolute value of the coefficients. This regularization term forces some of the coefficients to be exactly zero, effectively performing feature selection.\n",
    "\n",
    "Key differences from other regression techniques:\n",
    "\n",
    "Lasso vs. Ordinary Least Squares (OLS): Lasso introduces a regularization term that shrinks the coefficients of less important features, potentially eliminating them (making their coefficients zero). In contrast, OLS does not have any penalty term and will include all features.\n",
    "Lasso vs. Ridge Regression: Both Lasso and Ridge are regularization techniques. However, Ridge regression adds a penalty proportional to the square of the coefficients (L2 regularization), while Lasso adds a penalty proportional to the absolute value of the coefficients (L1 regularization). Lasso can shrink coefficients to zero, leading to feature selection, while Ridge only shrinks coefficients, but doesnt eliminate them.\n",
    "\n",
    "\n",
    "# Q2. What is the main advantage of using Lasso Regression in feature selection?\n",
    "The main advantage of Lasso Regression in feature selection is its ability to shrink some coefficients to exactly zero, effectively removing those features from the model. This results in a simpler, more interpretable model that retains only the most relevant features. In cases where there are many irrelevant or redundant features, Lasso helps improve the model's performance by excluding these unimportant variables.\n",
    "\n",
    "# Q3. How do you interpret the coefficients of a Lasso Regression model?\n",
    "Interpreting the coefficients in a Lasso Regression model is similar to interpreting coefficients in a standard linear regression model:\n",
    "\n",
    "Non-zero coefficients: The coefficient value indicates the change in the target variable for a one-unit change in the corresponding feature, assuming all other features are held constant.\n",
    "Zero coefficients: Features whose coefficients are shrunk to zero by Lasso are effectively excluded from the model, indicating that these features do not have a meaningful relationship with the target variable in the context of the other features.\n",
    "Since Lasso performs feature selection, the resulting model will only include the variables with non-zero coefficients, making it more interpretable.\n",
    "\n",
    "# Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?\n",
    "The primary tuning parameter in Lasso Regression is the regularization parameter \n",
    "\n",
    "位 (also called alpha in some libraries). This parameter controls the strength of the penalty applied to the coefficients.\n",
    "\n",
    "Effect of \n",
    "位:\n",
    "If \n",
    "\n",
    "=\n",
    "0\n",
    "位=0, Lasso behaves like Ordinary Least Squares regression with no regularization (no shrinkage).\n",
    "If \n",
    "\n",
    "位 is large, the penalty term dominates, and the model shrinks the coefficients more aggressively, potentially setting many of them to zero. This results in a simpler model but may lead to underfitting.\n",
    "Choosing an optimal \n",
    "\n",
    "位: A balance must be found. A small value of \n",
    "\n",
    "位 leads to overfitting, while a large value leads to underfitting.\n",
    "The choice of \n",
    "\n",
    "位 is critical to achieving an appropriate balance between model complexity and prediction accuracy. You typically select \n",
    "\n",
    "位 using cross-validation.\n",
    "\n",
    "# Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?\n",
    "Lasso Regression itself is designed for linear regression problems, where the relationship between the features and the target variable is assumed to be linear. However, Lasso can still be used for non-linear regression problems if you transform the features into a higher-dimensional space where the relationship becomes linear. This is typically done by:\n",
    "\n",
    "Polynomial features: Transforming the original features by adding higher-order terms (e.g., \n",
    "\n",
    "Kernel methods: Using techniques such as kernel trick (in kernelized regression models like SVM with non-linear kernels) to map the data to a higher-dimensional space where the relationship may become linear.\n",
    "After transforming the data, Lasso can be applied to the transformed features, making it suitable for handling non-linear relationships indirectly.\n",
    "\n",
    "\n",
    "# Q6. What is the difference between Ridge Regression and Lasso Regression?\n",
    "\n",
    "Ridge Regression and Lasso Regression are both regularized linear regression techniques, but they differ in how they apply regularization:\n",
    "\n",
    "Penalty term:\n",
    "\n",
    "Ridge: Uses L2 regularization, which penalizes the sum of the squares of the coefficients\n",
    "\n",
    "Lasso: Uses L1 regularization, which penalizes the absolute sum of the coefficients\n",
    "\n",
    "Feature selection:\n",
    "\n",
    "Ridge: Does not perform feature selection. It shrinks the coefficients but retains all features.\n",
    "Lasso: Performs feature selection by setting some coefficients exactly to zero.\n",
    "Multicollinearity:\n",
    "\n",
    "Ridge: Ridge is more effective in handling multicollinearity, as it reduces the impact of correlated features without eliminating them.\n",
    "Lasso: Lasso can help with multicollinearity by eliminating correlated features, but it might be less stable when there are many correlated predictors.\n",
    "\n",
    "\n",
    "# Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?\n",
    "Yes, Lasso Regression can handle multicollinearity to some extent, but it does so by removing some of the correlated features entirely. When there are highly correlated features, Lasso will tend to select one feature from the group and shrink the others to zero, effectively eliminating them from the model. This process helps reduce the instability caused by multicollinearity but comes at the cost of potentially excluding some useful predictors.\n",
    "\n",
    "However, if there are many correlated features, Lasso may not always perform well because it tends to arbitrarily choose between the correlated features. Ridge regression might be a better choice in such cases, as it does not eliminate features.\n",
    "\n",
    "Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?\n",
    "To choose the optimal value of \n",
    "\n",
    "位 in Lasso Regression, you typically perform the following steps:\n",
    "\n",
    "Cross-validation: Use k-fold cross-validation to test the model performance for different values of \n",
    "\n",
    "位 and select the value that minimizes the cross-validation error.\n",
    "Grid Search: Perform a grid search over a range of \n",
    "\n",
    "位 values (e.g., from very small values close to zero to larger values) to determine which \n",
    "\n",
    "位 gives the best performance.\n",
    "Validation Curve: You can plot a validation curve for different values of \n",
    "\n",
    "位 and choose the value where the model achieves the lowest validation error.\n",
    "The optimal value of \n",
    "\n",
    "位 ensures the best trade-off between bias and variance, preventing both underfitting and overfitting.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
